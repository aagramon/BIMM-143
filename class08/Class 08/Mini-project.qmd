---
title: "Mini-Project"
author: Arturo Agramont
format: pdf
editor: visual
---

## Mini-Project

We will start by reading the data

```{r}
csvfile= "WisconsinCancer.csv"
wisc.df = read.csv(csvfile, row.names = 1)
```

We will remove the first column

```{r}
wisc.data = wisc.df[,-1]
```

```{r}
diag_levels = c("B", "M")
diagnosis = factor(wisc.df$diagnosis, levels =diag_levels)
diagnosis
```

-   **Q1**. How many observations are in this dataset?

    ```{r}
    nrow(wisc.data)
    ```

    There are 569 observations in this data set.

-   **Q2**. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

212 observations were given a malignant diagnosis

-   **Q3**. How many variables/features in the data are suffixed with `_mean`?

```{r}
grep("_mean", wisc.df)
```

10 of the variables are suffixed with \_mean

# PCA

Checking columns and standard deviations

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr = prcomp(scale(wisc.data))

```

```{r}
summary(wisc.pr)
```

-   **Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

    0.4427 is the proportion of PC1.

-   **Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

    3 principle components are needed. PC1, PC2, and PC3

-   **Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

    7 PCs are needed to describe 90% of variance.\

We will now create a biplot

```{r}
biplot(wisc.pr)
```

-   **Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot does not make much sense and just appears to be a jumbled mess. It is difficult to understand.

Making a scatter plot of components 1 and 2

```{r}
plot(wisc.pr$x[,1:2], col = diagnosis , xlab = "PC1", ylab = "PC2")
```

-   **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

There is a greater distinction between points in plot 1 than plot 2

Now using ggplot2

```{r}
df = as.data.frame(wisc.pr$x)
df$diagnosis = diagnosis

library(ggplot2)

ggplot(df) + aes(PC1,PC2, col = diagnosis)+ geom_point()
```

Calculating variance of each principal component

```{r}
pr.var = wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve = pr.var/sum(pr.var)

plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim= c(0,1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

-   **Q9.** For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?

    ```{r}
    wisc.pr$rotation[,1]
    ```

It tells how much the original feature contributes to the first PC.

# Hierarchical clustering

First we will scale wisc.data and assign to data.scaled

```{r}
data.scaled = scale(wisc.data)
```

calculating euclidean distance between pairs in scaled data

```{r}
data.dist= dist(data.scaled)
```

Creating a hierarchical clustering model

```{r}
wisc.hclust = hclust(data.dist, method = "complete" )
```

-   **Q10.** Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline( h=19, col="red", lty = 2)
```

Using function cutree() in order to make the tree have 4 clusters

```{r}
wisc.hclust.clusters = cutree(wisc.hclust, k = 4, h=19)

```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

**Q12.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning

```{r}
wisc.hclust.single = hclust(data.dist, method = "single" )
wisc.hclust.average = hclust(data.dist, method = "average" )
wisc.hclust.ward = hclust(data.dist, method = "ward.D2" )
plot(wisc.hclust.average)
plot(wisc.hclust.single)
plot(wisc.hclust.ward)
```

Either ward or complete are my favorite results for the same data set because they provide easier to digest views of the data..

Now we will look into the 2 groups of the ward tree

```{r}
data_dist = dist(wisc.pr$x[,1:7])
wisc.pr.hclust = hclust(data_dist, method = "ward.D2" )
grps = cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g = as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

-   **Q13**. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(grps, diagnosis)
```

\
It seperates the diagnoses more where diagnosis B is more in group 2 and diagnosis M is more in group 1.

**Q14**. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and `wisc.hclust.clusters`) with the vector containing the actual diagnoses

```{r}
table(wisc.hclust.clusters, diagnosis)

```

This created 4 groups in place of 2 that don't have a more distinct formation of groups as the other model.

# Prediction

Using predict() function to take PCA model from before onto new cancer data

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col= g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

-   **Q16.** Which of these new patients should we prioritize for follow up based on your results?

1 should be prioritzed
